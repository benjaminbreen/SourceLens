// pages/api/summarize-draft.ts
// API endpoint for generating structured summaries of user draft documents
// Ensures fullText is extracted and included in the sections array.
// FIX: Asks LLM to generate descriptive titles for each section.
// FIX: Increased MAX_SECTIONS to 10.

import type { NextApiRequest, NextApiResponse } from 'next';
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';
import { getModelById } from '@/lib/models'; // Assuming this utility exists

// --- Configuration ---
const DEFAULT_MODEL_ID = 'gemini-flash-lite';
const MAX_SECTIONS = 10; // Increased max sections
const MIN_SECTION_LENGTH = 150;
const MAX_SNIPPET_LENGTH = 2500; // Max characters per section snippet sent to LLM
const PROMPT_CHAR_LIMIT = 300000;
const MAX_OUTPUT_TOKENS = 8192;

// Configure Google client
let googleAI: GoogleGenerativeAI | null = null;
try {
    if (!process.env.GOOGLE_API_KEY) {
        throw new Error("GOOGLE_API_KEY environment variable is not set.");
    }
    googleAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
} catch (e) {
    console.error("Failed to initialize GoogleGenerativeAI:", e);
}


// Interface for the structure the API should return
interface SummaryApiResponse {
  overallSummary: string;
  sections: Array<{
    id: string;
    title: string;     // Title generated/chosen by LLM
    summary: string;   // Summary generated by LLM
    fullText: string; // The actual text content of this section
  }>;
  wordCount?: number;
  wasTruncated?: boolean;
  chunkCount?: number;
}

// --- Helper function to split text into logical sections ---
// Returns sections with a *heuristic* title, fullText, and a temporary ID.
function splitIntoSections(
    text: string,
    maxSections: number = MAX_SECTIONS,
    minSectionLength: number = MIN_SECTION_LENGTH
): Array<{ title: string; fullText: string; id: string }> {
    let potentialSections: string[] = [];
    const tripleSplit = text.split(/\n\n\n+/);
    if (tripleSplit.length > 1 && tripleSplit.length <= maxSections * 1.5) {
        potentialSections = tripleSplit;
        console.log(`Split by triple newline yielded ${potentialSections.length} potential sections.`);
    } else {
        potentialSections = text.split(/\n\n+/);
        console.log(`Split by double newline yielded ${potentialSections.length} potential sections.`);
    }

    return potentialSections
        .map(s => s.trim())
        .filter(s => s.length >= minSectionLength)
        .slice(0, maxSections)
        .map((sectionText, index) => {
            const firstLine = sectionText.split('\n')[0].trim();
            const seemsLikeHeading = firstLine.length > 0 && firstLine.length < 80 && !/[.!?]$/.test(firstLine);
            // Provide a basic title, LLM will improve/replace it
            const title = seemsLikeHeading ? firstLine : `Section ${index + 1}`;
            return {
                id: `section-${index + 1}`,
                title: title, // Basic title
                fullText: sectionText
            };
        });
}

// Base part of the prompt template (defined outside handler)
// --- UPDATED PROMPT ---
const summaryPromptBase = `
You are DraftSummarizerAI, an expert academic assistant. Your task is to analyze a document pre-divided into sections and provide a structured summary in JSON format.

DOCUMENT TITLE: {draftTitle}

SECTIONS (ID, Basic Title, and Text Snippet provided):
{sectionsText}

TASK:
1.  Write a brief overall summary (2-3 sentences) capturing the main purpose and argument of the entire document based on the provided sections.
2.  For EACH section provided above:
    a.  Analyze the Text Snippet.
    b.  Generate a concise and descriptive TITLE (3-7 words) that accurately reflects the section's main topic. If the provided basic title is already good, you can use it, otherwise, create a better one. Do not just use "Section N".
    c.  Generate a concise 1-2 sentence SUMMARY capturing the key points of the section's Text Snippet.

FORMAT YOUR RESPONSE AS VALID JSON:
{
  "overallSummary": "Your 2-3 sentence overview goes here",
  "sections": [
    {
      "id": "section-1", // Use the ID from the input section
      "title": "Generated Descriptive Title for Section 1", // Your generated title
      "summary": "Concise summary of section 1 generated by you"
    },
    {
      "id": "section-2", // Use the ID from the input section
      "title": "Generated Descriptive Title for Section 2", // Your generated title
      "summary": "Concise summary of section 2 generated by you"
    }
    // ... include an object with id, title, and summary for EACH input section
  ]
}

IMPORTANT:
- Return ONLY valid JSON. No introductory text, explanations, or markdown formatting.
- Ensure a section object exists in your JSON output for every section ID provided in the input.
- Use the exact 'id' from the input for each corresponding section object in your output.
- Generate meaningful 'title' and 'summary' values for each section.
`;

// --- Main API Handler ---
export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<SummaryApiResponse | { message: string; error?: string; rawResponse?: string }>
) {
  if (!googleAI) {
      return res.status(500).json({ message: "AI Client not initialized. Check API Key." });
  }
  if (req.method !== 'POST') {
    res.setHeader('Allow', ['POST']);
    return res.status(405).json({ message: 'Method not allowed' });
  }

  console.log("Summarize Draft API called");

  try {
    const { draft, modelId = DEFAULT_MODEL_ID } = req.body;

    if (!draft || typeof draft.content !== 'string' || draft.content.trim() === '') {
      return res.status(400).json({ message: 'Draft content is required.' });
    }
    const draftContent = draft.content;
    const draftTitle = typeof draft.title === 'string' && draft.title.trim() ? draft.title.trim() : 'Untitled Draft';

    console.log(`Processing summary: "${draftTitle.substring(0, 30)}..." (${draftContent.length} chars)`);

    let modelConfig;
    try { modelConfig = getModelById(modelId); }
    catch (error) {
      console.warn(`Model ID "${modelId}" invalid: ${error}. Falling back.`);
      modelConfig = getModelById(DEFAULT_MODEL_ID);
    }
    console.log(`Using model: ${modelConfig.name} (${modelConfig.apiModel})`);

    const wordCount = draftContent.split(/\s+/).filter(Boolean).length;
    console.log(`Word count: ${wordCount}`);

    // --- 1. Section Splitting ---
    const sectionsWithFullText = splitIntoSections(draftContent);
    if (sectionsWithFullText.length === 0) {
        console.warn("Could not split draft into sections.");
        sectionsWithFullText.push({ id: 'section-1', title: draftTitle || 'Full Document', fullText: draftContent });
    }
    console.log(`Identified ${sectionsWithFullText.length} sections.`);

    // --- 2. Prepare LLM Prompt ---
    const sectionsForLLMPrompt = sectionsWithFullText.map(sec => ({
      id: sec.id,
      title: sec.title, // Pass the basic title
      textSnippet: sec.fullText.substring(0, MAX_SNIPPET_LENGTH) + (sec.fullText.length > MAX_SNIPPET_LENGTH ? '...' : '')
    }));

    const sectionsTextForPrompt = sectionsForLLMPrompt
      .map((sec, i) => `SECTION ${i + 1} (ID: ${sec.id})\nTITLE: ${sec.title}\nTEXT SNIPPET:\n${sec.textSnippet}\n---`)
      .join('\n\n');

    const finalSummaryPrompt = summaryPromptBase
        .replace('{draftTitle}', draftTitle)
        .replace('{sectionsText}', sectionsTextForPrompt);

    const estimatedPromptLength = finalSummaryPrompt.length;
    console.log(`Estimated prompt length: ~${estimatedPromptLength} characters`);

    if(estimatedPromptLength > PROMPT_CHAR_LIMIT) {
         return res.status(413).json({ message: `Draft too large (Prompt size: ${estimatedPromptLength} chars).` });
    }

    // --- 3. Call LLM for Summaries AND Titles ---
    const model = googleAI.getGenerativeModel({
        model: modelConfig.apiModel,
        safetySettings: [ /* ... safety settings ... */
            { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
            { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
            { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
            { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
        ],
     });

    console.log(`Calling Gemini (${modelConfig.apiModel}) with max output ${MAX_OUTPUT_TOKENS}...`);
    let result;
    try {
        const generationConfig = {
            temperature: 0.25, // Slightly higher temp might help title generation
            maxOutputTokens: MAX_OUTPUT_TOKENS,
            responseMimeType: "application/json",
        };
        result = await model.generateContent({
            contents: [{ role: 'user', parts: [{ text: finalSummaryPrompt }] }],
            generationConfig: generationConfig,
        });
    } catch (llmError: any) { /* ... existing LLM error handling ... */
         console.error("!!! Google API generateContent Error:", llmError);
         const detail = llmError.message || 'Unknown LLM API error';
         const status = llmError.status || 500;
         return res.status(status).json({ message: `LLM generation failed: ${detail}`, error: JSON.stringify(llmError) });
    }

    const response = result.response;
    const finishReason = response?.promptFeedback?.blockReason || response?.candidates?.[0]?.finishReason;
    const finishMessage = response?.promptFeedback?.blockReason
        ? `Blocked: ${response.promptFeedback.blockReason} - ${response.promptFeedback.blockReasonMessage || ''}`
        : `Finished: ${finishReason || 'UNKNOWN'}`;
    console.log(`LLM Finish Reason: ${finishMessage}`);

    if (response?.promptFeedback?.blockReason) { /* ... existing safety block handling ... */
        console.error(`LLM generation blocked: ${response.promptFeedback.blockReason}`, response.promptFeedback);
        const blockMessage = response.promptFeedback.blockReasonMessage || response.promptFeedback.blockReason;
        return res.status(400).json({ message: `Content generation blocked: ${blockMessage}`, error: `Safety Filter: ${response.promptFeedback.blockReason}`});
    }
    if (finishReason && finishReason !== "STOP" && finishReason !== "MAX_TOKENS") { /* ... existing finish reason handling ... */
        console.error(`LLM generation finished unexpectedly: ${finishReason}`);
         return res.status(500).json({ message: `Generation stopped unexpectedly: ${finishReason}.`, error: `Finish Reason: ${finishReason}` });
     }
     if (finishReason === "MAX_TOKENS") { console.warn("LLM generation may have been truncated."); }

    const responseText = response?.text();
    if (!responseText && !response?.candidates?.[0]?.content?.parts?.[0]?.text) {
         throw new Error('LLM returned no parsable response content.');
    }
    console.log(`LLM Response received. Attempting JSON parse...`);

    // --- JSON Extraction and Parsing ---
    let jsonString = '';
    let llmSummaryData;
    try {
        const jsonPart = response?.candidates?.[0]?.content?.parts?.find(part => part.text?.trim().startsWith('{'));
        if (jsonPart && jsonPart.text) {
            jsonString = jsonPart.text.trim();
             if (jsonString.startsWith('```json')) { jsonString = jsonString.substring(7); }
             if (jsonString.endsWith('```')) { jsonString = jsonString.substring(0, jsonString.length - 3); }
             jsonString = jsonString.trim();
        } else if (responseText) {
            const jsonMatch = responseText.match(/\{[\s\S]*\}/);
            if (!jsonMatch || !jsonMatch[0]) throw new Error('No valid JSON object braces found.');
            jsonString = jsonMatch[0];
        } else { throw new Error('No content found in LLM response.'); }

         jsonString = jsonString.replace(/\t/g, ' ').replace(/[\x00-\x09\x0b\x0c\x0e-\x1f\x7f]/g, '');
        console.log("Attempting to parse potentially sanitized JSON string...");
        llmSummaryData = JSON.parse(jsonString);
        console.log("JSON parsed successfully.");
    } catch (parseError) { /* ... existing parse error handling ... */
      console.error('JSON parse error in summary API:', parseError);
      console.error("--- Raw LLM Response Text ---"); console.error(responseText || "N/A");
      console.error("--- JSON String Attempted ---"); console.error(jsonString); console.error("--- End ---");
      throw new Error(`Failed to parse JSON from LLM: ${parseError instanceof Error ? parseError.message : String(parseError)}`);
    }

    // --- 4. Combine LLM Results with Original Sections ---
    if (!llmSummaryData || !Array.isArray(llmSummaryData.sections)) {
        throw new Error("Parsed LLM data structure is invalid. Expected 'sections' array.");
    }

    const finalSections = sectionsWithFullText.map(originalSection => {
        const llmSectionResult = llmSummaryData.sections.find((s: any) => s && s.id === originalSection.id);
        // Use LLM title/summary if valid, otherwise fallback gracefully
        const finalTitle = (typeof llmSectionResult?.title === 'string' && llmSectionResult.title.trim() !== '')
                           ? llmSectionResult.title.trim()
                           : originalSection.title; // Fallback to heuristic title
        const finalSummary = (typeof llmSectionResult?.summary === 'string' && llmSectionResult.summary.trim() !== '')
                             ? llmSectionResult.summary.trim()
                             : "[Summary not generated]"; // Clearer fallback

        return {
            id: originalSection.id,
            title: finalTitle,
            summary: finalSummary,
            fullText: originalSection.fullText // Keep the fullText from our split
        };
    });

    if (finalSections.length !== sectionsWithFullText.length) {
         console.warn(`Section count mismatch. Original: ${sectionsWithFullText.length}, Final: ${finalSections.length}.`);
    }
    console.log(`Summary combination complete. Final sections count: ${finalSections.length}`);

    // --- 5. Return the Complete Structure ---
    const finalResponse: SummaryApiResponse = {
        overallSummary: llmSummaryData.overallSummary || "[Overall summary not generated]",
        sections: finalSections,
        wordCount,
    };

    return res.status(200).json(finalResponse);

  } catch (error) { // Outer catch block
    console.error('Draft summarization API top-level error:', error);
    return res.status(500).json({
      message: 'Error processing draft summarization request',
      error: error instanceof Error ? error.message : 'Unknown error'
    });
  }
}

// --- API Config (Moved outside handler) ---
export const config = {
  api: {
    bodyParser: { sizeLimit: '10mb' },
    responseLimit: false,
  },
};